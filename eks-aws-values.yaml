secretproviderclass:
  enabled: true
  SecretProviderName: "erpnext-aws-secrets"
  SecretProvider: "aws"
  secretName: "db-credentials"
  secrets:
    - objectName: "dev/erpnext/database"  # This is your secret name in AWS Secrets Manager
      keys:
        - path: "dbRootPassword"
          value: "db-root-password"

# Default values


# Configure external database host
# If we provide an external dbHost , it will not create standalone db
dbHost: "erpnext-dev.cejra4hw3o2e.ap-south-1.rds.amazonaws.com"
dbPort: 3306
dbRootUser: "admin"

# If we provide dbRootPassword ,it will create a secret for dbRootPassword
dbRootPassword: "498KUoT9uty2P7a9MgSD"
dbRds: true

nameOverride: "erpnext" # this will pe used as label eg.app.kubernetes.io/name: {{ include "erpnext.name" . }}-worker-l
fullnameOverride: "frappe-bench-erpnext"  # this will be used as a prefix for the resource name 

image:
  repository:  harshjoundal7/helpdesk
  tag: amd
  pullPolicy: IfNotPresent

# Any new site added in sitesConfig will be created while new-site jobs runs automatically
# on same instance.
# Frappe supports multi-tenant architecture, so we can have multiple sites in same instance
sitesConfig:
  - siteName: erp.harshjoundal.in
    enabled: true
    dbName: harsh-erpnext-aws # db name for site should be unique as it will be created in same cluster
    forceCreate: false # do not force create if site aleady exist , it will override and create new site
    #These apps will be installed while site creation 
    #also if we add new apps in future,it will install those apps
    # while site migration 
    installApps:
      - erpnext
      - special-ops-frappe-utilities
    uninstallApps: #These apps will be uninstalled when uninstallApps job runs
      - helpdesk
    backup: # If enabled site will be backed up while backup job runs
      enabled: false
  # - siteName: erp.cluster.local2
  #   enabled: false
  #   dbName: erpnext-test-local-2
  #   forceCreate: false # do not force create if site aleady exist , it will override and create new site
  #   installApps: #These apps will be installed while migration
  #     - helpdesk
  #   uninstallApps:
  #     - helpdesk
  #   backup:
  #     enabled: false

commonEnvVars: &commonEnvVars
  - name: PP_BACKEND_BASE_URL
    value: "http://penpencil-backend-service.microservices.svc"
  - name: BATCH_MICROSERVICE_BASE_URL
    value: "http://batches-service.microservices.svc"
  - name: USER_MICROSERVICE_BASE_URL
    value: "http://user-microservice.microservices.svc"
  - name: AWS_REGION
    value: "ap-south-1"
  - name: AWS_BUCKET
    value: "special-ops-erpnext-dev"

nginx:
  replicaCount: 2
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    targetCPU: 75
    targetMemory: 75
  # config: |
  #   # custom conf /etc/nginx/conf.d/default.conf
  environment:
    upstreamRealIPAddress: "127.0.0.1"
    upstreamRealIPRecursive: "off"
    upstreamRealIPHeader: "X-Forwarded-For"
    frappeSiteNameHeader: "$host"
  livenessProbe:
    tcpSocket:
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 10
  readinessProbe:
    tcpSocket:
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 10
  service:
    type: ClusterIP
    port: 8080
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  envVars: []
  initContainers: []
  sidecars: []

worker:
  gunicorn:
    replicaCount: 2
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 75
      targetMemory: 75
    livenessProbe:
      tcpSocket:
        port: 8000
      initialDelaySeconds: 5
      periodSeconds: 10
    readinessProbe:
      tcpSocket:
        port: 8000
      initialDelaySeconds: 5
      periodSeconds: 10
    service:
      type: ClusterIP
      port: 8000
    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
    args: []
    envVars: []
    initContainers: []
    sidecars: []

  default:
    replicaCount: 1
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 75
      targetMemory: 75
    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
    livenessProbe:
      override: false
      probe: {}
    readinessProbe:
      override: false
      probe: {}
    envVars: []
    initContainers: []
    sidecars: []

  short:
    replicaCount: 1
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 75
      targetMemory: 75
    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
    livenessProbe:
      override: false
      probe: {}
    readinessProbe:
      override: false
      probe: {}
    envVars: []
    initContainers: []
    sidecars: []

  long:
    replicaCount: 1
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 75
      targetMemory: 75
    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
    livenessProbe:
      override: false
      probe: {}
    readinessProbe:
      override: false
      probe: {}
    envVars: []
    initContainers: []
    sidecars: []

  scheduler:
    replicaCount: 1
    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
    livenessProbe:
      override: false
      probe: {}
    readinessProbe:
      override: false
      probe: {}
    envVars: []
    initContainers: []
    sidecars: []

  healthProbe: |
    exec:
      command:
        - bash
        - -c
        - echo "Ping backing services";
        {{- if .Values.mariadb.enabled }}
        {{- if eq .Values.mariadb.architecture "replication" }}
        - wait-for-it {{ .Release.Name }}-mariadb-primary:{{ .Values.mariadb.primary.service.ports.mysql }} -t 1;
        {{- else }}
        - wait-for-it {{ .Release.Name }}-mariadb:{{ .Values.mariadb.primary.service.ports.mysql }} -t 1;
        {{- end }}
        {{- else if .Values.dbHost }}
        - wait-for-it {{ .Values.dbHost }}:{{ .Values.mariadb.primary.service.ports.mysql }} -t 1;
        {{- end }}
        {{- if index .Values "redis-cache" "enabled" }}
        - wait-for-it {{ .Release.Name }}-redis-cache-master:{{ index .Values "redis-cache" "master" "containerPorts" "redis" }} -t 1;
        {{- else if index .Values "redis-cache" "host" }}
        - wait-for-it {{ index .Values "redis-cache" "host" }} -t 1;
        {{- end }}
        {{- if index .Values "redis-queue" "enabled" }}
        - wait-for-it {{ .Release.Name }}-redis-queue-master:{{ index .Values "redis-queue" "master" "containerPorts" "redis" }} -t 1;
        {{- else if index .Values "redis-queue" "host" }}
        - wait-for-it {{ index .Values "redis-queue" "host" }} -t 1;
        {{- end }}
    initialDelaySeconds: 15
    periodSeconds: 5

socketio:
  replicaCount: 2
  autoscaling:
    enabled: false
    minReplicas: 2
    maxReplicas: 3
    targetCPU: 75
    targetMemory: 75
  livenessProbe:
    tcpSocket:
      port: 9000
    initialDelaySeconds: 5
    periodSeconds: 10
  readinessProbe:
    tcpSocket:
      port: 9000
    initialDelaySeconds: 5
    periodSeconds: 10
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  service:
    type: ClusterIP
    port: 9000
  envVars: []
  initContainers: []
  sidecars: []

# PVC configuration
persistence:
  worker:
    enabled: true
    accessModes:
      - ReadWriteMany
    # existingClaim: ""
    size: 8Gi
    storageClass: "efs-sc"

# Ingress
ingress:
  # ingressName: ""
  # className: ""
  enabled: true
  annotations: 
    traefik.ingress.kubernetes.io/router.entrypoints: web
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    # cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
  - host: erp.harshjoundal.in
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
  #  - secretName: auth-server-tls
  #    hosts:
  #      - auth-server.local

jobs:
  volumePermissions:
    enabled: false
    backoffLimit: 0
    resources: {}
    annotations: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}

  configure:
    enabled: true
    fixVolume: true
    backoffLimit: 0
    resources: {}
    annotations: 
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "0"
      
      # ArogoCd doesnt works well with helm hooks, use below for local testing
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "1"
      # "helm.sh/hook-delete-policy": "before-hook-creation"
    nodeSelector: {}
    tolerations: []
    affinity: {}
    envVars: []
    command: []
    args: []
    extraArgs: 
      - bench set-config -g allow_cors "*"

  createSite:
    enabled: true
    forceCreate: true
    siteName: "erp.cluster.local"
    adminPassword: "changeit"
    dbType: "mariadb"
    backoffLimit: 0
    resources: {}
    annotations: 
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "1"
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "1"
      # "helm.sh/hook-delete-policy": "before-hook-creation"
    nodeSelector: {}
    tolerations: []
    affinity: {}

  dropSite:
    enabled: false
    forced: false
    siteName: "erp.cluster.local2"
    backoffLimit: 0
    resources: {}
    annotations: 
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "3"
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "3"
      # "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
    nodeSelector: {}
    tolerations: []
    affinity: {}

  backup:
    enabled: true
    withFiles: true
    backoffLimit: 0
    resources: {}
    annotations: 
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "2"
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "3"
      # "helm.sh/hook-delete-policy": "before-hook-creation"
    nodeSelector: {}
    tolerations: []
    affinity: {}

  migrate:
    enabled: true
    skipFailing: false
    # postMigrateCommands:
    #   - echo "---Running post-migration commands---"
    #   - bench build
    backoffLimit: 0
    annotations:
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "5"
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "6"
      # "helm.sh/hook-delete-policy": "before-hook-creation"
    resources: 
      requests:
        cpu: 2000m
        memory: 2Gi
      limits:
        cpu: 2000m
        memory: 2.5Gi
    nodeSelector: {}
    tolerations: []
    affinity: {}

  custom:
    enabled: false
    jobName: ""
    labels: {}
    backoffLimit: 0
    initContainers: []
    containers: []
    restartPolicy: Never
    annotations: {}
    volumes: []
    nodeSelector: {}
    affinity: {}
    tolerations: []

  uninstallApps:
    enabled: false
    jobName: ""
    backoffLimit: 0
    forced: false
    resources: {}
    annotations: 
      "argocd.argoproj.io/hook": "Sync"
      "argocd.argoproj.io/sync-wave": "4"
      "argocd.argoproj.io/hook-delete-policy": "HookSucceeded,BeforeHookCreation"
      # "helm.sh/hook": "post-install,post-upgrade"
      # "helm.sh/hook-weight": "4"
      # "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
    nodeSelector: {}
    tolerations: []
    affinity: {}

imagePullSecrets: []


# ----------------------  Service Account  ----------------------
# This servie
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  annotations: {}

podSecurityContext:
  supplementalGroups: [1000]

securityContext:
  capabilities:
    add:
    - CAP_CHOWN
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# ----------------------  REDIS  ----------------------
# If we are using external redis , disable this and provide the host so it will not create standalone redis
redis-cache:
  # host: "redis://k8s-redis-development-001.5wulab.0001.aps1.cache.amazonaws.com:6379"
  enabled: true # If we are using external redis , disable this so it will not create standalone redis
  architecture: standalone
  auth:
    enabled: false
    sentinal: false
  master:
    containerPorts:
      redis: 6379
    persistence:
      enabled: false

redis-queue:
  # host: "redis://k8s-redis-development-001.5wulab.0001.aps1.cache.amazonaws.com:6379"
  # https://github.com/bitnami/charts/tree/master/bitnami/redis
  enabled: true # If we are using external redis , disable this so it will not create standalone redis
  architecture: standalone
  auth:
    enabled: false
    sentinal: false
  master:
    containerPorts:
      redis: 6379
    persistence:
      enabled: false

# --------------------- DB ----------------------
# If we are using external db , disable this so it will not create standalone db
mariadb:
  # https://github.com/bitnami/charts/tree/master/bitnami/mariadb
  enabled: false
  auth:
    rootPassword: "changeit"
    username: "erpnext"
    password: "changeit"
    replicationPassword: "changeit"
  primary:
    service:
      ports:
        mysql: 3306
    extraFlags: >-
      --skip-character-set-client-handshake
      --skip-innodb-read-only-compressed
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci
